{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Esn7C4TW2Iq"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from collections import OrderedDict\n",
        "import os\n",
        "\n",
        "# Add CRAFT-pytorch repository to the Python path\n",
        "import sys\n",
        "sys.path.append('/content/CRAFT-pytorch')\n",
        "\n",
        "from craft import CRAFT\n",
        "from craft_utils import getDetBoxes, adjustResultCoordinates\n",
        "from imgproc import loadImage, normalizeMeanVariance\n",
        "from file_utils import saveResult"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/clovaai/CRAFT-pytorch.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYHnywXhYmhl",
        "outputId": "266e9045-4dff-4a08-9bd1-50d3a5f55c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CRAFT-pytorch'...\n",
            "remote: Enumerating objects: 59, done.\u001b[K\n",
            "remote: Total 59 (delta 0), reused 0 (delta 0), pack-reused 59\u001b[K\n",
            "Receiving objects: 100% (59/59), 1.69 MiB | 25.84 MiB/s, done.\n",
            "Resolving deltas: 100% (25/25), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd CRAFT-pytorch/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7JT2fFHYvC5",
        "outputId": "3d653734-57bd-4c9a-b019-8bcd065e55ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CRAFT-pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Edit the file craft.py\n",
        "!sed -i 's/from torchvision.models.vgg import model_urls//' basenet/vgg16_bn.py\n",
        "!sed -i 's/model_urls\\[.*\\]/\"https:\\/\\/download.pytorch.org\\/models\\/vgg16_bn-6c64b313.pth\"/' basenet/vgg16_bn.py\n",
        "\n"
      ],
      "metadata": {
        "id": "uGpivIqyc2XV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KM77TJ1YrQU",
        "outputId": "2e3cbca0-901e-4856-9a16-297fef92d9d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basenet\t\t   craft_utils.py  imgproc.py\tREADME.md\t  test.py\n",
            "craft_mlt_25k.pth  figures\t   LICENSE\trefinenet.py\n",
            "craft.py\t   file_utils.py   __pycache__\trequirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision\n",
        "!pip install opencv-python\n",
        "!pip install gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiaiwsqQbU88",
        "outputId": "41809abc-e1f6-4c5f-b271-283a3da03ea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.14.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.6.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1Jk4eGD7crsqCCg9C9VjCLkMN3ze8kutZ -O craft_mlt_25k.pth\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8whBdd3aSlv",
        "outputId": "e139a74c-f60c-4f07-e43d-bae9e4eef4aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Jk4eGD7crsqCCg9C9VjCLkMN3ze8kutZ\n",
            "To: /content/CRAFT-pytorch/craft_mlt_25k.pth\n",
            "100% 83.2M/83.2M [00:01<00:00, 67.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from collections import OrderedDict\n",
        "import os\n",
        "# Add CRAFT-pytorch repository to the Python path\n",
        "import sys\n",
        "sys.path.append('/content/CRAFT-pytorch')\n",
        "from craft import CRAFT\n",
        "from craft_utils import getDetBoxes\n",
        "from imgproc import normalizeMeanVariance\n",
        "from file_utils import saveResult\n",
        "\n",
        "# Helper functions\n",
        "def copyStateDict(state_dict):\n",
        "    if list(state_dict.keys())[0].startswith(\"module\"):\n",
        "        start_idx = 1\n",
        "    else:\n",
        "        start_idx = 0\n",
        "    new_state_dict = OrderedDict()\n",
        "    for k, v in state_dict.items():\n",
        "        name = \".\".join(k.split(\".\")[start_idx:])\n",
        "        new_state_dict[name] = v\n",
        "    return new_state_dict\n",
        "\n",
        "def load_craft_model(model_path):\n",
        "    net = CRAFT()  # initialize\n",
        "    net.load_state_dict(copyStateDict(torch.load(model_path, map_location='cpu')))\n",
        "    net.eval()\n",
        "    return net\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    return image\n",
        "\n",
        "def detect_text_regions(image, net):\n",
        "    x = normalizeMeanVariance(image)\n",
        "    x = torch.from_numpy(x).permute(2, 0, 1)\n",
        "    x = x.unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        y, _ = net(x)\n",
        "\n",
        "    score_text = y[0, :, :, 0].cpu().data.numpy()\n",
        "    score_link = y[0, :, :, 1].cpu().data.numpy()\n",
        "\n",
        "    # Lower thresholds to capture more of the line\n",
        "    boxes, _ = getDetBoxes(score_text, score_link, 0.4, 0.3, 0.3)\n",
        "    return boxes\n",
        "\n",
        "def merge_boxes_to_lines(boxes, max_y_diff=15, x_padding=140):\n",
        "    if len(boxes) <= 1:\n",
        "        return boxes\n",
        "\n",
        "    boxes = sorted(boxes, key=lambda x: (x[0][1] + x[1][1] + x[2][1] + x[3][1]) / 4)  # Sort by average y-coordinate\n",
        "    merged_boxes = []\n",
        "    current_line = boxes[0]\n",
        "\n",
        "    for box in boxes[1:]:\n",
        "        current_y = (current_line[0][1] + current_line[3][1]) / 2\n",
        "        box_y = (box[0][1] + box[3][1]) / 2\n",
        "\n",
        "        if abs(box_y - current_y) <= max_y_diff:\n",
        "            # Merge boxes\n",
        "            x_coords = [p[0] for b in (current_line, box) for p in b]\n",
        "            y_coords = [p[1] for b in (current_line, box) for p in b]\n",
        "            current_line = [\n",
        "                [min(x_coords), min(y_coords)],\n",
        "                [max(x_coords), min(y_coords)],\n",
        "                [max(x_coords), max(y_coords)],\n",
        "                [min(x_coords), max(y_coords)]\n",
        "            ]\n",
        "        else:\n",
        "            merged_boxes.append(current_line)\n",
        "            current_line = box\n",
        "\n",
        "    merged_boxes.append(current_line)\n",
        "\n",
        "    # Add padding to each side of the box\n",
        "    padded_boxes = []\n",
        "    for box in merged_boxes:\n",
        "        padded_box = [\n",
        "            [box[0][0] - x_padding, box[0][1]],\n",
        "            [box[1][0] + x_padding, box[1][1]],\n",
        "            [box[2][0] + x_padding, box[2][1]],\n",
        "            [box[3][0] - x_padding, box[3][1]]\n",
        "        ]\n",
        "        padded_boxes.append(padded_box)\n",
        "\n",
        "    return padded_boxes\n",
        "\n",
        "def crop_text_lines(image, boxes, output_dir):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    for i, box in enumerate(boxes):\n",
        "        box = np.array(box).astype(np.int32)\n",
        "        x_min = max(0, np.min(box[:, 0]))\n",
        "        x_max = min(image.shape[1] - 1, np.max(box[:, 0]))\n",
        "        y_min = max(0, np.min(box[:, 1]))\n",
        "        y_max = min(image.shape[0] - 1, np.max(box[:, 1]))\n",
        "        crop_img = image[y_min:y_max, x_min:x_max]\n",
        "\n",
        "        # Only save if the cropped image is not empty\n",
        "        if crop_img.size > 0:\n",
        "            cv2.imwrite(f\"{output_dir}/line_{i}.png\", cv2.cvtColor(crop_img, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "# Main Function\n",
        "def segment_text_lines(image_path, model_path, output_dir):\n",
        "    # Load image\n",
        "    image = preprocess_image(image_path)\n",
        "\n",
        "    # Load CRAFT model\n",
        "    net = load_craft_model(model_path)\n",
        "\n",
        "    # Detect text regions\n",
        "    boxes = detect_text_regions(image, net)\n",
        "\n",
        "    # Merge boxes to form lines\n",
        "    line_boxes = merge_boxes_to_lines(boxes)\n",
        "\n",
        "    # Crop and save text lines\n",
        "    crop_text_lines(image, line_boxes, output_dir)\n",
        "\n",
        "# Specify paths\n",
        "image_path = \"/content/1.png\"\n",
        "model_path = \"/content/CRAFT-pytorch/craft_mlt_25k.pth\"\n",
        "output_dir = \"/content/output_directory2\"\n",
        "\n",
        "# Run the text line segmentation\n",
        "segment_text_lines(image_path, model_path, output_dir)"
      ],
      "metadata": {
        "id": "-PD97xoby6g7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from collections import OrderedDict\n",
        "import os\n",
        "\n",
        "# Add CRAFT-pytorch repository to the Python path\n",
        "import sys\n",
        "sys.path.append('/content/CRAFT-pytorch')\n",
        "\n",
        "from craft import CRAFT\n",
        "from craft_utils import getDetBoxes\n",
        "from imgproc import normalizeMeanVariance\n",
        "from file_utils import saveResult\n",
        "\n",
        "# Helper functions\n",
        "def copyStateDict(state_dict):\n",
        "    if list(state_dict.keys())[0].startswith(\"module\"):\n",
        "        start_idx = 1\n",
        "    else:\n",
        "        start_idx = 0\n",
        "    new_state_dict = OrderedDict()\n",
        "    for k, v in state_dict.items():\n",
        "        name = \".\".join(k.split(\".\")[start_idx:])\n",
        "        new_state_dict[name] = v\n",
        "    return new_state_dict\n",
        "\n",
        "def load_craft_model(model_path):\n",
        "    net = CRAFT()  # initialize\n",
        "    net.load_state_dict(copyStateDict(torch.load(model_path, map_location='cpu')))\n",
        "    net.eval()\n",
        "    return net\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        raise ValueError(f\"Image at path {image_path} could not be loaded.\")\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    return image\n",
        "\n",
        "def detect_text_regions(image, net):\n",
        "    x = normalizeMeanVariance(image)\n",
        "    x = torch.from_numpy(x).permute(2, 0, 1)\n",
        "    x = x.unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        y, _ = net(x)\n",
        "\n",
        "    score_text = y[0, :, :, 0].cpu().data.numpy()\n",
        "    score_link = y[0, :, :, 1].cpu().data.numpy()\n",
        "\n",
        "    # Lower thresholds to capture more of the line\n",
        "    boxes, _ = getDetBoxes(score_text, score_link, 0.4, 0.3, 0.3)\n",
        "    return boxes\n",
        "\n",
        "def merge_boxes_to_lines(boxes, max_y_diff=15, x_padding=140):\n",
        "    if len(boxes) <= 1:\n",
        "        return boxes\n",
        "\n",
        "    boxes = sorted(boxes, key=lambda x: (x[0][1] + x[1][1] + x[2][1] + x[3][1]) / 4)  # Sort by average y-coordinate\n",
        "    merged_boxes = []\n",
        "    current_line = boxes[0]\n",
        "\n",
        "    for box in boxes[1:]:\n",
        "        current_y = (current_line[0][1] + current_line[3][1]) / 2\n",
        "        box_y = (box[0][1] + box[3][1]) / 2\n",
        "\n",
        "        if abs(box_y - current_y) <= max_y_diff:\n",
        "            # Merge boxes\n",
        "            x_coords = [p[0] for b in (current_line, box) for p in b]\n",
        "            y_coords = [p[1] for b in (current_line, box) for p in b]\n",
        "            current_line = [\n",
        "                [min(x_coords), min(y_coords)],\n",
        "                [max(x_coords), min(y_coords)],\n",
        "                [max(x_coords), max(y_coords)],\n",
        "                [min(x_coords), max(y_coords)]\n",
        "            ]\n",
        "        else:\n",
        "            merged_boxes.append(current_line)\n",
        "            current_line = box\n",
        "\n",
        "    merged_boxes.append(current_line)\n",
        "\n",
        "    # Add padding to each side of the box\n",
        "    padded_boxes = []\n",
        "    for box in merged_boxes:\n",
        "        padded_box = [\n",
        "            [box[0][0] - x_padding, box[0][1]],\n",
        "            [box[1][0] + x_padding, box[1][1]],\n",
        "            [box[2][0] + x_padding, box[2][1]],\n",
        "            [box[3][0] - x_padding, box[3][1]]\n",
        "        ]\n",
        "        padded_boxes.append(padded_box)\n",
        "\n",
        "    return padded_boxes\n",
        "\n",
        "def crop_text_lines(image, boxes, output_dir):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    for i, box in enumerate(boxes):\n",
        "        box = np.array(box).astype(np.int32)\n",
        "        x_min = max(0, np.min(box[:, 0]))\n",
        "        x_max = min(image.shape[1] - 1, np.max(box[:, 0]))\n",
        "        y_min = max(0, np.min(box[:, 1]))\n",
        "        y_max = min(image.shape[0] - 1, np.max(box[:, 1]))\n",
        "        crop_img = image[y_min:y_max, x_min:x_max]\n",
        "\n",
        "        # Only save if the cropped image is not empty\n",
        "        if crop_img.size > 0:\n",
        "            cv2.imwrite(f\"{output_dir}/line_{i}.png\", cv2.cvtColor(crop_img, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "# Main Function\n",
        "def segment_text_lines(image_path, model_path, output_dir):\n",
        "    # Load image\n",
        "    image = preprocess_image(image_path)\n",
        "\n",
        "    # Load CRAFT model\n",
        "    net = load_craft_model(model_path)\n",
        "\n",
        "    # Detect text regions\n",
        "    boxes = detect_text_regions(image, net)\n",
        "\n",
        "    # Merge boxes to form lines\n",
        "    line_boxes = merge_boxes_to_lines(boxes)\n",
        "\n",
        "    # Crop and save text lines\n",
        "    crop_text_lines(image, line_boxes, output_dir)\n",
        "\n",
        "# Specify paths\n",
        "image_path = \"/content/1.png\"  # Update this to your image path\n",
        "model_path = \"/content/CRAFT-pytorch/craft_mlt_25k.pth\"\n",
        "output_dir = \"/content/output_directory3\"\n",
        "\n",
        "# Run the text line segmentation\n",
        "segment_text_lines(image_path, model_path, output_dir)\n"
      ],
      "metadata": {
        "id": "qv0trcDe8rKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from collections import OrderedDict\n",
        "import os\n",
        "\n",
        "# Add CRAFT-pytorch repository to the Python path\n",
        "import sys\n",
        "sys.path.append('/content/CRAFT-pytorch')\n",
        "\n",
        "from craft import CRAFT\n",
        "from craft_utils import getDetBoxes\n",
        "from imgproc import normalizeMeanVariance\n",
        "from file_utils import saveResult\n",
        "\n",
        "# Helper functions\n",
        "def copyStateDict(state_dict):\n",
        "    if list(state_dict.keys())[0].startswith(\"module\"):\n",
        "        start_idx = 1\n",
        "    else:\n",
        "        start_idx = 0\n",
        "    new_state_dict = OrderedDict()\n",
        "    for k, v in state_dict.items():\n",
        "        name = \".\".join(k.split(\".\")[start_idx:])\n",
        "        new_state_dict[name] = v\n",
        "    return new_state_dict\n",
        "\n",
        "def load_craft_model(model_path):\n",
        "    net = CRAFT()  # initialize\n",
        "    net.load_state_dict(copyStateDict(torch.load(model_path, map_location='cpu')))\n",
        "    net.eval()\n",
        "    return net\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        raise ValueError(f\"Image at path {image_path} could not be loaded.\")\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    return image\n",
        "\n",
        "def detect_text_regions(image, net):\n",
        "    x = normalizeMeanVariance(image)\n",
        "    x = torch.from_numpy(x).permute(2, 0, 1)\n",
        "    x = x.unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        y, _ = net(x)\n",
        "\n",
        "    score_text = y[0, :, :, 0].cpu().data.numpy()\n",
        "    score_link = y[0, :, :, 1].cpu().data.numpy()\n",
        "\n",
        "    # Lower thresholds to capture more of the line\n",
        "    boxes, _ = getDetBoxes(score_text, score_link, 0.2, 0.2, 0.2)\n",
        "    return boxes\n",
        "\n",
        "def merge_boxes_to_lines(boxes, image_height=1200,image_w=500, max_y_diff_upper=15, max_y_diff_lower=10):\n",
        "    if len(boxes) <= 1:\n",
        "        return boxes\n",
        "\n",
        "    boxes = sorted(boxes, key=lambda x: (x[0][1] + x[1][1] + x[2][1] + x[3][1]) / 4)  # Sort by average y-coordinate\n",
        "    merged_boxes = []\n",
        "    current_line = boxes[0]\n",
        "\n",
        "    for box in boxes[1:]:\n",
        "        current_y = (current_line[0][1] + current_line[3][1]) / 2\n",
        "        box_y = (box[0][1] + box[3][1]) / 2\n",
        "\n",
        "        if box_y < image_height / 2:\n",
        "            max_y_diff = max_y_diff_upper\n",
        "        else:\n",
        "            max_y_diff = max_y_diff_lower\n",
        "\n",
        "        if abs(box_y - current_y) <= max_y_diff:\n",
        "            # Merge boxes\n",
        "            x_coords = [p[0] for b in (current_line, box) for p in b]\n",
        "            y_coords = [p[1] for b in (current_line, box) for p in b]\n",
        "            current_line = [\n",
        "                [min(x_coords), min(y_coords)],\n",
        "                [max(x_coords), min(y_coords)],\n",
        "                [max(x_coords), max(y_coords)],\n",
        "                [min(x_coords), max(y_coords)]\n",
        "            ]\n",
        "        else:\n",
        "            merged_boxes.append(current_line)\n",
        "            current_line = box\n",
        "\n",
        "    merged_boxes.append(current_line)\n",
        "\n",
        "    # Adjust the boxes to span the full width of the image\n",
        "    image_width = image_w\n",
        "    full_width_boxes = []\n",
        "    for box in merged_boxes:\n",
        "        # Convert the box to a NumPy array for slicing\n",
        "        box_array = np.array(box)\n",
        "        y_min = np.min(box_array[:, 1]) # Now you can use slicing\n",
        "        y_max = np.max(box_array[:, 1])\n",
        "        full_width_box = np.array([\n",
        "            [0, y_min],\n",
        "            [image_width - 1, y_min],\n",
        "            [image_width - 1, y_max],\n",
        "            [0, y_max]\n",
        "        ])\n",
        "        full_width_boxes.append(full_width_box)\n",
        "\n",
        "    return full_width_boxes\n",
        "\n",
        "# Main Function\n",
        "def segment_text_lines(image_path, model_path, output_dir):\n",
        "    # Load image\n",
        "    image = preprocess_image(image_path)\n",
        "\n",
        "    # Load CRAFT model\n",
        "    net = load_craft_model(model_path)\n",
        "\n",
        "    # Detect text regions\n",
        "    boxes = detect_text_regions(image, net)\n",
        "\n",
        "    # Merge boxes to form lines\n",
        "    line_boxes = merge_boxes_to_lines(boxes)\n",
        "\n",
        "    # Crop and save text lines\n",
        "    crop_text_lines(image, line_boxes, output_dir)\n",
        "\n",
        "# Specify paths\n",
        "image_path = \"/content/1.png\"  # Update this to your image path\n",
        "model_path = \"/content/CRAFT-pytorch/craft_mlt_25k.pth\"\n",
        "output_dir = \"/content/output_directory7\"\n",
        "\n",
        "# Run the text line segmentation\n",
        "segment_text_lines(image_path, model_path, output_dir)\n"
      ],
      "metadata": {
        "id": "MQo1p1D-loJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HsPVrcbnnc5B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}